---
title: "srs_exp3_analysis"
author: "--"
date: "6/22/2020"
output: pdf_document
---

```{r}
library(lme4)
library(dplyr)
library(scales)
library(ggplot2)
```

## Extracting IDs from demographics data

Prior to reading in the data, there was some data cleaning that is documented in `data_cleaning_steps.txt`. There was some repetition of participant ID numbers as well as some errors in participants not matching the inclusion criteria (i.e. native language).

```{r}
#setwd("~/Documents/Psychology/Labs/LCNL/Research/current/grammatical_memory/paper/supplementary_materials/experiment_3")
d_demo = read_csv("srs_exp3_demographics_final.csv")
```

We want to extract all ID numbers from this demograhpics data to ensure we match all of the experimental data. We will also need to remove participants that are non-native speakers of English from the experimental data prior to completing any analyses.

```{r}
ids = d_demo$id
```

## Formatting experimental data

```{r}
d = read_csv("srs_exp3_data.csv")
```

First, let's remove participants that are non-native speakers. The ID numbers in the `ids` variable only contain those who indicated they are native English speakers. First, we transform the ID numbers into a human readable format (removing some string formatting from unicode), and then we remove the excess ID numbers.

```{r}
length(unique(d$participant_id))

d = d %>%
  mutate(participant_id = as.numeric(str_extract(participant_id, "[0-9]+"))) %>%
  filter(participant_id %in% ids)

length(unique(d$participant_id))
```

Our numbers now match the demographics data. We can move on with the rest of our analyses.

The data has largely already been formatted in initial storage and through Python scripting prior to getting to this R Markdown file. Here, we apply a few transformations to get the data to its final format.

The transformations are as follows:
* Factorizing many variables
* `pres_cond`: Conditions for the grammatical regularities of the presented pair
* `pres_cond_str`: String format of `pres_cond`
* `test_cond`: Conditions for the grammatical regularities of the tested pair
* `test_cond_str`: String format of `test_cond`
* `old_cond`: Condition tracking whether the pair is Old or New
* `old_cond_str`: String format of `old_cond`
* `resp_old`: Formatting whether the participant said the pair was old
* `resp_old_part_rate`: Overall response rate. Participants with high or low response rates are subsequently removed.
* Participants who miss all three attention check are also removed
* `c_art`: Centered ART score
* `c_judgment`: Centered judgment for each participant

The conditions are indicated by the number in the `condition` column. Here is the interpretation of the codes:
* 1 -- Presented in a consistent order; tested in a consistent order; target response OLD
* 2 -- Presented in a consistent order; tested in a reversed order; target response NEW
* 3 -- Presented in a reversed order; tested in a consistent order; target response NEW
* 4 -- Presented in a reversed order; tested in a reversed order; target response OLD

```{r}
d = d %>%
  mutate(participant_id = as.factor(participant_id),
         group_id = as.factor(participant_id),
         orig_pair = as.factor(orig_pair),
         pres_cond = ifelse(condition %in% c(1, 2), 0.5, -0.5),
         pres_cond_str = factor(pres_cond, 
                                levels = c(-0.5, 0.5), 
                                labels = c('Inconsistent', 'Consistent')),
         test_cond = ifelse(condition %in% c(1, 3), 0.5, -0.5),
         test_cond_str = factor(test_cond, 
                                levels = c(-0.5, 0.5), 
                                labels = c('Inconsistent', 'Consistent')),
         old_cond = ifelse(condition %in% c(1, 4), 0.5, -0.5),
         old_cond_str = as.factor(ifelse(condition %in% c(1, 4), 'Old', 'New')),
         resp_old = ifelse(response %in% c('old'), 1, 0),
         resp_old_part_rate = ave(resp_old, participant_id)) %>%
  filter((resp_old_part_rate <= 0.95) & (resp_old_part_rate >= 0.05)) %>%
  filter((study_check == 1) | (test_check == 1) | (ART_check == 1))%>%
  mutate(c_judgment = judgment - ave(judgment, participant_id),
         c_art = ART - mean(ART),
         art_m_split = as.factor(if_else(c_art > 0, "High ART", "Low ART")),
         art_m_split = fct_rev(art_m_split))
```

We removed a total of 6 participants with our filters.

```{r}
length(unique(d$participant_id))
```

Transformations look good.

```{r}
describe(d$c_judgment)

describe(d$c_art)
```

Extract final ids

```{r}
ids = d$participant_id
```

## Demographics data

We need to parse down to only the participants that are used for analysis.

```{r}
d_demo = d_demo %>%
  filter(id %in% ids)
```

```{r}
# Age
d_demo %>%
  select(age) %>%
  summarise(min(age, na.rm = TRUE),
            mean(age, na.rm = TRUE),
            max(age, na.rm = TRUE),
            sd(age,na.rm = TRUE))
```

```{r}
# Gender
d_demo %>%
  select(gender) %>%
  table()
```

```{r}
# Education
d_demo %>%
  select(highest_education) %>%
  table()
```

```{r}
# Race
d_demo %>%
  select(race) %>%
  table()
```

## Analyses

First, let's look at some brief descriptive statistics.

### Effect of presented pair grammaticality on old ratings

If participants use grammatical LTM information to encode novel pairs, then we would expect participants to be biased toward recognizing pairs that are consistent with the LTM of syntactic structures. Thus, in terms of this experiment, we would expect the pairs that were presented in a consistent order (i.e. typical NM -- typical HN) to be more likely to be called Old than pairs that were presented in an inconsistent order (i.e. typical HN -- typical NM) regardless of the order the pair is presented in later.

```{r}
d %>%
  group_by(pres_cond_str) %>%
  summarise(mean(resp_old),
            se(resp_old))
```

Likely to not find any difference.

```{r}
d %>%
  ggplot(aes(x = pres_cond_str, y = resp_old)) +
  stat_summary(geom = 'bar', width = 0.25) + 
  stat_summary(fun.data = mean_cl_boot, geom = 'errorbar', alpha = 0.50, width = 0.10) +
  scale_y_continuous(limits = c(0.0, 1.0)) +
  labs(x = "Presentation condition",
       y = "Average old rating",
       title = "Effect of presentation grammaticality on old rating",
       subtitle = "Collapsed across all participants")
```

### Effects of test pair grammaticality on old ratings

Using the same theory as above, we would also expect participants to be biased toward calling pairs tested in a consistent order to be more likely to be called Old than pairs that were tested in an inconsistent order.

```{r}
d %>%
  ggplot(aes(x = test_cond_str, y = resp_old)) +
  stat_summary(geom = 'bar', width = 0.25) + 
  stat_summary(fun.data = mean_cl_boot, geom = 'errorbar', alpha = 0.50, width = 0.10) +
  scale_y_continuous(limits = c(0.0, 1.0)) +
  labs(x = "Test condition",
       y = "Average old rating",
       title = "Effect of test grammaticality on old rating",
       subtitle = "Collapsed across all participants")
```

### Effect of old-ness on old ratings

Finally, we would expect old ratings to be supported by the actual age of the pair. Pairs that are old should be rated as old more frequently.

```{r}
d %>%
  ggplot(aes(x = old_cond_str, y = resp_old)) +
  stat_summary(geom = 'bar', width = 0.25) + 
  stat_summary(fun.data = mean_cl_boot, geom = 'errorbar', alpha = 0.50, width = 0.10) +
  scale_y_continuous(limits = c(0.0, 1.0)) +
  labs(x = "Oldness of pair",
       y = "Average old rating",
       title = "Effect of age on old rating",
       subtitle = "Collapsed across all participants")
```

### Effect of test pair grammaticality and oldness on old ratings

We can also look at these effects together.

```{r}
d %>%
  group_by(old_cond_str, test_cond_str) %>%
  summarise(mean_old = mean(resp_old),
            se_old = se(resp_old))
```

```{r}
overall_recog_inter = d %>%
  ggplot(aes(x = old_cond_str, 
             y = resp_old,
             color = test_cond_str,
             fill = test_cond_str)) +
  stat_summary(geom = 'bar',
               position = position_dodge(width = 0.5), 
               alpha = 0.50, width = 0.5) + 
  stat_summary(fun.data = mean_cl_boot, 
               geom = 'errorbar',
               position = position_dodge(width = 0.5),
               alpha = 1, width = 0.10)+
  scale_y_continuous(limits = c(0, 1.05),
                     breaks = c(0.0, 0.25, 0.50, 0.75, 1.0),
                     expand = c(0, 0)) +
  labs(x = "Oldness of pair",
       y = "Average old rating",
       fill = "Test condition",
       title = "Effect of age and test condition on old rating",
       subtitle = "Descriptive stats collapsed across all participants") +
  guides(color = FALSE) + 
  scale_color_manual(values = c("#99A8B1", "#EB5E55")) +
  scale_fill_manual(values = c("#99A8B1", "#EB5E55"))

overall_recog_inter
```

Let's also add participant data in to this plot.

```{r}
participant_recog_inter = d %>%
  group_by(old_cond_str, test_cond_str, participant_id) %>%
  summarise(mean_old = mean(resp_old),
            se_old = se(resp_old))

overall_recog_inter +
  geom_point(data = participant_recog_inter,
             aes(x = old_cond_str,
                 y = mean_old,
                 color = test_cond_str),
             position = position_jitterdodge(jitter.width = 0.2,
                                             jitter.height = 0.05,
                                             dodge.width = .51,
                                             seed = 9),
             alpha = 0.25)
```

Clearly, there are huge differences between participants, but the overall trend is pretty clear. Participants are more likely to call old pairs old, and compounds that are tested in an order consistent with long-term experience are more likely to be called old than compounds that are tested in an order inconsistent with long-term experience.

### Looking at order of compound at study and order of compound at test

Looking at order of compound at study and order of compound at test is just another of way of looking at a combination of oldness of the compound and order of the compound at test, but it's still useful to visualize in this way

```{r}
d %>%
  ggplot(aes(x = pres_cond_str, 
             y = resp_old,
             color = test_cond_str,
             fill = test_cond_str)) +
  stat_summary(geom = 'bar',
               position = position_dodge(width = 0.5), 
               alpha = 0.50, width = 0.5) + 
  stat_summary(fun.data = mean_cl_boot, 
               geom = 'errorbar',
               position = position_dodge(width = 0.5),
               alpha = 1, width = 0.10) +
  scale_y_continuous(limits = c(0, 1.05),
                     breaks = c(0.0, 0.25, 0.50, 0.75, 1.0),
                     expand = c(0, 0)) +
  labs(x = "Study condition",
       y = "Average old rating",
       fill = "Test condition",
       title = "Effect of age and test condition on old rating",
       subtitle = "Descriptive stats collapsed across all participants") +
  guides(color = FALSE) +
  scale_color_manual(values = c("#99A8B1", "#EB5E55")) +
  scale_fill_manual(values = c("#99A8B1", "#EB5E55"))
```

### Looking at order of compound at study and oldness of compound

```{r}
d %>%
  ggplot(aes(x = pres_cond_str, 
             y = resp_old,
             color = old_cond_str,
             fill = old_cond_str)) +
  stat_summary(geom = 'bar',
               position = position_dodge(width = 0.5), 
               alpha = 0.50, width = 0.5) + 
  stat_summary(fun.data = mean_cl_boot, 
               geom = 'errorbar',
               position = position_dodge(width = 0.5),
               alpha = 1, width = 0.10) +
  scale_y_continuous(limits = c(0, 1.05),
                     breaks = c(0.0, 0.25, 0.50, 0.75, 1.0),
                     expand = c(0, 0)) +
  labs(x = "Study condition",
       y = "Average old rating",
       fill = "Oldness of compound",
       title = "Effect of study condition and test condition on old rating",
       subtitle = "Descriptive stats collapsed across all participants") +
  guides(color = FALSE) +
  scale_color_manual(values = c("#99A8B1", "#EB5E55")) +
  scale_fill_manual(values = c("#99A8B1", "#EB5E55"))
```

### Combining effects of oldness, consistency at time of test, and ART score

This is the Figure we are thinking of using for the paper. For the paper, we are going to estimate the means and CIs from the data using `stat_summary`.

```{r}
d %>%
  ggplot(aes(x = old_cond_str, 
             y = resp_old,
             color = test_cond_str,
             fill = test_cond_str)) +
  facet_wrap(~art_m_split) +
  stat_summary(geom = 'bar',
               position = position_dodge(width = 0.5), 
               alpha = 0.50, width = 0.5) + 
  stat_summary(fun.data = mean_cl_boot, 
               geom = 'errorbar',
               position = position_dodge(width = 0.5),
               alpha = 1, width = 0.10) +
  scale_y_continuous(limits = c(0, 1.05),
                     breaks = c(0.0, 0.25, 0.50, 0.75, 1.0),
                     expand = c(0, 0)) +
  labs(x = "Oldness of pair",
       y = "Average old rating",
       fill = "Test condition") +
  guides(color = FALSE) +
  scale_fill_branded(target = "Atomic tangerine") +
  scale_colour_branded() +
  theme(plot.background = element_rect(fill = 'white', colour = 'white')) +
  theme(panel.background = element_rect(fill = 'white', colour = 'white'))
```

### Effect of meaningfulness on old ratings

We might also expect pairs that are rated as more meaningful to be called old more frequently, assumign that participants are regenerating their memory from a combination of semantic and syntactic factors. Now, this is not a pure measurement of anything; participants are explicitly asked to generate these meaningfulness ratings, so it could be the case that participants who are rating pairs higher are thinking more deeply about the task. Perhpas there is a depth-of-processing component to the scores we will see. Either way, I think it is interesting.

We should look at this both across the presentation condition (Consistent/Reversed) as well as at the individual item level. We might expect those pairs that are in the consistent order to be rated as more highly meaningful. We might expect to see this both within individual pairs and across all pairs.

```{r}
# Across all pairs
d %>%
  group_by(pres_cond_str) %>%
  summarise(mean_rating = mean(judgment),
            se_rating = mean(judgment))

# Individual pairs
d %>%
  group_by(orig_pair, pres_cond_str) %>%
  summarise(mean_rating = mean(judgment),
            se_rating = se(judgment))
```

We see a trend in the direction across all items, though there is huge variability there. Let's look at the difference for each pair.

```{r}
rating_diffs = d %>%
  group_by(orig_pair, pres_cond_str) %>%
  summarise(mean_rating = mean(judgment)) %>%
  pivot_wider(names_from = pres_cond_str, values_from = mean_rating) %>%
  ungroup() %>%
  mutate(diff_rating = Consistent - Reversed)

rating_diffs
```

In this table, a positive value in `diff_rating` would indicate a higher rating for the consistent ordering compared to a reversed ordering for a given pair while a negative value in `diff_rating` would indicate a lower rating for a consistent ordering compared to a reversed ordering.

Let's look at some of the pairs that are *inconsistent* with our hypotheses, that pairs in a consistent ordering will be rated as more meaningful than pairs that are in the reversed ordering.

```{r}
rating_diffs %>%
  arrange(diff_rating) %>%
  print(n = 20)
```

Some of these pairs make a lot of sense being rated higher in the reversed order. For example, the noun `consumer` in `consumer song` is likely interpreted as a verb when in the reversed order of `song consumer`. Other pairs, like `fork video`, are probably heavily influenced by recent trends in video consumption shaping the distributional patterns of the nouns used in this experiment that were not captured in my corpus analyses.

Now, let's look at the pairs that are *consistent* with our hypotheses. In this case, we are looking at pairs that are rated as more meaningful in the consistent ordering than the reverse ordering.

```{r}
rating_diffs %>%
  arrange(desc(diff_rating)) %>%
  print(n = 20)
```

Overall, there is a strong trend in this direction. The rating differences are greater than the previous sample.

Some of the highest rated compounds are as follows:

```{r}
rating_diffs %>%
  arrange(desc(Consistent)) %>%
  print(n = 20)

rating_diffs %>%
  arrange(desc(Reversed)) %>%
  print(n = 20)
```

Let's do a quick visualization of the ratings.

* P.S. [Here](https://community.rstudio.com/t/pivot-longer-on-multiple-column-sets-pairs/43958/9) is a really helpful solution to pivoting pairs of columns using the .value specification.

```{r}
# Get each pair's rating from participants
d_individ_pairs = d %>%  
  group_by(orig_pair, pres_cond_str) %>%
  summarise(mean_rating = mean(judgment),
            se_rating = se(judgment)) %>%
  pivot_wider(names_from = pres_cond_str, 
              values_from = c(mean_rating, se_rating)) %>%
  ungroup() %>%
  mutate(consis_more_meaning = if_else(mean_rating_Consistent > mean_rating_Inconsistent, 
                               TRUE, 
                               FALSE),
         diff_rating = mean_rating_Consistent - mean_rating_Inconsistent) %>%
  pivot_longer(cols = c(mean_rating_Consistent,
                        mean_rating_Inconsistent,
                        se_rating_Consistent,
                        se_rating_Inconsistent),
               names_to = c(".value", "pres_cond_str"),
               names_pattern = "(.*_.*)_(.*)")

# Plot overall ratings for ddifferent conditions
d %>%
  ggplot(aes(x = pres_cond_str, y = judgment, 
             fill = pres_cond_str, color = pres_cond_str)) +
  stat_summary(geom = "bar",
               width = .5,
               alpha = .75) +
  stat_summary(geom = 'errorbar',
               fun.data = mean_cl_boot,
               alpha = 1, width = 0.10) +
  geom_point(data = d_individ_pairs, 
             aes(x = pres_cond_str, y = mean_rating, color = pres_cond_str), 
             alpha = .25,
             position = position_jitterdodge(jitter.width = 0.1,
                                             jitter.height = 0,
                                             dodge.width = .1,
                                             seed = 9)) +
  scale_y_continuous(expand = c(0, 0),
                     breaks = c(1, 2, 3, 4, 5),
                     limits = c(1, 5),
                     oob = rescale_none) +
  labs(title = "Rating of pairs by presentation condition",
       subtitle = "Averaged over all participants",
       x = "Grammatical regularity of pair",
       y = "Mean rating") +
    guides(color = FALSE,
           fill = FALSE)
```

### Merging the data frames

These ratings will be merged with the data for the entire experiment to be used as a covariate in some models.

```{r}
d_ratings = d %>%
  group_by(orig_pair, pres_cond_str) %>%
  summarise(mean_rating = mean(judgment))
```

Finally, we need to center the ratings. The ratings are mean-centered for each participant.

```{r}
d_final = d %>%
  left_join(y = d_ratings, by = c("orig_pair", "pres_cond_str")) %>%
  mutate(mean_rating_c = mean_rating - ave(mean_rating, participant_id))

describe(d_final$mean_rating_c)
```

## Modeling

We are interested in analyzing two parts of this experiment. First, we care about the old ratings of the pairs when they are presented at test. Second, we care about the meaningfulness ratings of the pairs. 

### Predicting old ratings

According to the [registration](https://osf.io/4m6vz)* of this study, we planned to run an a binomial repeated measurs logistic regression. Our fixed effects include actual oldness of the pair (`old_cond`), the grammaticality of the pair at test (`test_cond`), the ART score of the participant (`c_art`), the interaction between the oldness of the pair and participant ART score (`old_cond:c_art`), and the interaction between the grammaticality of the pair at rest and the participant ART score (`test_cond:c_art`).

The full model would include multiple random effects: a by-participant random intercept, a by-participant random slope for `old_cond`, a by-participant random slope for `test_cond`, a by-item random intercept, a by-item random slope for `old_cond`, a by-item random slope for `test_cond`, a by-item random slope for `c_art`, a by-item random slope for the interaction `old_cond:c_art`, a by-item random slope for the interaction `test_cond:c_art`.

Fitting all of these random effects has proven to be difficult. To get a converging model, some of these random effects had to be removed. These include the interaction slopes (`old_cond:c_art` and `test_cond:c_art`) and the slope for the ART score (`c_art`). Models with these random affects were attempted ot be fit. A variety of optimizers were used, and the maximum number of iterations was increased.

* Note, this pre-registration was made following the collection of the data but prior to any analysis of the data, including looking at descriptive statistics of any kind. All hypotheses were formed based on a separate pilot sample

```{r}
m = glmer(resp_old ~ 
            old_cond + 
            test_cond + 
            c_art + 
            old_cond:c_art +
            test_cond:c_art + 
            (1 + old_cond + test_cond|participant_id) + 
            (1 + old_cond + test_cond + c_art|orig_pair), 
          data = d_final, 
          family = binomial, 
          control = glmerControl(optCtrl = list(maxfun=5e5), 
                                 optimizer = 'bobyqa'))
summary(m)
Anova(m, type = 3)
```

```{r}
m_all_main = glmer(resp_old ~ 
                   old_cond + 
                   pres_cond +
                   test_cond + 
                  (1 + old_cond + pres_cond + test_cond|participant_id) + 
                  (1 + old_cond + pres_cond + test_cond|orig_pair), 
                  data = d_final, 
                  family = binomial, 
                  control = glmerControl(optCtrl = list(maxfun=5e5), 
                                         optimizer = 'bobyqa'))
summary(m_all_main)
Anova(m_all_main, type = 3)
```

```{r}
# Fails to converge
m_inter = glmer(resp_old ~
                  pres_cond +
                  old_cond +
                  pres_cond:old_cond +
                  (1 + pres_cond + old_cond|participant_id) +
                  (1 + pres_cond + old_cond|orig_pair),
                data = d_final, 
                family = binomial, 
                control = glmerControl(optCtrl = list(maxfun=5e5), 
                                       optimizer = 'bobyqa'))
summary(m_inter)
Anova(m_inter, type = 3)
```

```{r}
m_study = glmer(resp_old ~
                  pres_cond +
                  test_cond +
                  pres_cond:test_cond +
                  (1|participant_id) +
                  (1|orig_pair),
                data = d_final, 
                family = binomial, 
                control = glmerControl(optCtrl = list(maxfun=5e5), 
                                       optimizer = 'Nelder_Mead'))
summary(m_study)
Anova(m_study, type = 3)
```

#### Can we account for the effect of grammaticality by including the judgments of participants?

To what extent are the effects of grammaticality dependent upon the semantics or meaningfulness of the pair of words in their given order? My expectation is that there is no clear answer, nor any clear way to separate grammaticality from meaningfulness, but we can begin to approach this question by statistically controlling by the average rating of the meaningfulness of the pair of words that they judged.

This model had some issues with singular fit. Including by-participant and by-item random slopes for `c_judgment` resulted in some poorly fit models.

```{r}
m_judg = glmer(resp_old ~
                 old_cond +
                 test_cond +
                 c_art +
                 c_judgment +
                 old_cond:c_art +
                 test_cond:c_art +
               (1 + old_cond + test_cond|participant_id) + 
               (1 + old_cond + test_cond + c_art|orig_pair), 
               data = d_final, 
               family = binomial, 
               control = glmerControl(optCtrl = list(maxfun=5e5), 
                                      optimizer = 'bobyqa'))
summary(m_judg)
Anova(m_judg, type = 3)
```

As we can see, when controlling for rating (but excluding rating random slopes!), the effect still persists. Ratings are also positively associated with old ratings, as higher ratings lead to more old ratings later.

In this next model, we use the average rating of a pair instead of each participant's rating of the pair they saw. Some participants rated different pairs in a different order (e.g. gas faith) than what they saw at the time of the recognition test (e.g. faith gas). The meaningfulness ratings of each participant therefore may not be meaningful for the conditions in which the order was reversed compared to when it was initially rated.

As you will see, this results in a singular fit (with bobyqa optimization function) or convergence issues (with Nelder Mead optimization function).

```{r}
m_judg_part = glmer(resp_old ~
                    old_cond +
                    test_cond +
                    c_art +
                    mean_rating_c +
                    old_cond:c_art +
                    test_cond:c_art +
                   (1 + old_cond + test_cond|participant_id) + 
                   (1 + old_cond + test_cond + c_art|orig_pair), 
                   data = d_final, 
                   family = binomial, 
                   control = glmerControl(optCtrl = list(maxfun=5e5), 
                                          optimizer = 'Nelder_Mead'))
summary(m_judg_part)
Anova(m_judg_part, type = 3)
```

#### Are judgments of the meaningfulness predictable from grammaticality of a pair?

We also be interested in characterizing how participants rate pairs. For our outcome, we are going to try and predict the rating from the grammaticality of the pair, as qwll as participant ART score.

```{r}
m_judg_pair = lmer(judgment ~ 
                     pres_cond + 
                     c_art +
                     pres_cond:c_art +
                  (1 + pres_cond|participant_id) +
                  (1 + pres_cond|orig_pair),
                  data = d,
                  control = lmerControl(optCtrl = list(maxfun=5e5), 
                                        optimizer = 'Nelder_Mead'))
summary(m_judg_pair)
Anova(m_judg_pair, type = 3)
```